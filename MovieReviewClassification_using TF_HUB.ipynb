{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MovieReviewClassification_FINALMODEL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameervalki/Machine-Learning/blob/ML-Basics/MovieReviewClassification_using%20TF_HUB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX6RobOpZieb",
        "colab_type": "text"
      },
      "source": [
        "#Movie Review Classifier\n",
        "\n",
        "\n",
        "```\n",
        "TEAM ASA\n",
        "```\n",
        "\n",
        "<BR>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdfOI23zvN_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing necessary libraries\n",
        "\n",
        "import pandas as pd   # For the DataFrame \n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib notebook                      # In jupyter run this line to get an interactive plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx9rTaGKJ-ST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k10KwN7Q1-nI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating the DataFrame\n",
        "df=pd.read_csv('/content/drive/My Drive/colabutils/mrcproject/IMDB Dataset.csv')    #For others, import the dataset by replacing the path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxXKDkedraZB",
        "colab_type": "text"
      },
      "source": [
        "<b><i>Exploratory Data Analysis on the Dataset</i></b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g5n2ag4CNnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df   #The complete DataFrame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLa3b4rl2Y-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KiaWaaa2ay8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9SGU9E06h9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To view a single sample\n",
        "\n",
        "list(df.loc[np.random.randint(0,1000)].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtGpT4-65609",
        "colab_type": "text"
      },
      "source": [
        "<br><b><i>WordCloud of the Dataset</i></b><br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y1KpFuRaY-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To See the wordcloud of both the sentiments - Positive and negative\n",
        "\n",
        "import re                                      #Regex library used for removing spcl characters and for clean sentences\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "STOPWORDS.add('movie')\n",
        "STOPWORDS.add('one')\n",
        "\n",
        "def dispwc():\n",
        "  \n",
        "  dg=df.groupby('sentiment')\n",
        "  neg=dg.get_group('negative').drop('sentiment',axis=1).values\n",
        "  pos=dg.get_group('positive').drop('sentiment',axis=1).values\n",
        "\n",
        "  wc1=WordCloud(width=800,height=500,background_color=\"white\",max_words=400,\n",
        "              random_state=None, min_font_size=1,prefer_horizontal=0.7)\n",
        "  wc2=WordCloud(width=800,height=500,background_color=\"black\",max_words=400,\n",
        "              random_state=None, min_font_size=1,prefer_horizontal=0.7)\n",
        "  \n",
        "  posstr=' '.join([x for x in ' '.join(re.split(r'\\W+',str(str(pos).split()))).split() if not x in STOPWORDS and x != 'br'])\n",
        "  negstr=' '.join([x for x in ' '.join(re.split(r'\\W+',str(str(neg).split()))).split() if not x in STOPWORDS and x != 'br'])\n",
        "\n",
        "  wc1.generate(posstr)\n",
        "  wc2.generate(negstr)\n",
        "\n",
        "  plt.figure(figsize=(40,80))\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(wc1)\n",
        "  plt.xlabel('Postive Reviews')\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(wc2)\n",
        "  plt.xlabel('Negative Reviews')\n",
        "\n",
        "\n",
        "#dispwc()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUGI_-wNaoKy",
        "colab_type": "text"
      },
      "source": [
        "#Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQgPRAgIHxis",
        "colab_type": "text"
      },
      "source": [
        "<br><b><i>Feature Extraction</i></b><br>\n",
        "We used Google News Text embedding from tensorflow hub. It converts Sentences into a 20 dimensional vector \n",
        "<br><br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLuPfTrUDOlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url=\"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
        "\n",
        "#url=\"https://tfhub.dev/google/universal-sentence-encoder/4\"         #512 dimensional embedding\n",
        "\n",
        "embed=hub.load(url)\n",
        "\n",
        "#We can use-  embed(['sentence']) to get the embeddings\n",
        "\n",
        "print(embed(['This is just a sample sentence, to show the text embeddings of any given sentence']).numpy(), end='\\n\\n')\n",
        "\n",
        "print(embed([\"The sentences can include even html tags, characters and numbers like- &nbsp; &, 65, <br>, * \"]).numpy(),end='\\n\\n')\n",
        "\n",
        "print(embed([\"The sentences can include even html tags characters and numbers like nbsp 65 br \"]).numpy(), end='\\n\\n') \n",
        "\n",
        "print('\"Observe the similarity of embeddings of the last two sents.\" ^')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgcNOEzxC_iH",
        "colab_type": "text"
      },
      "source": [
        "<br><b><i>Dataset creation</i></b><br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLce_NYmHfCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2=df.copy()                                     #Creating a copy for a possible future use"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHDFxW0hIy9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels=df.pop('sentiment').values\n",
        "reviews=df['review'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNfnz_TMdsjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_ser=pd.Series(labels, dtype='category')    # Its a categorical series. That is it converts all the values.\n",
        "                                                  # within that series into numbers\n",
        "                                                  \n",
        "newlabel=labels_ser.cat.codes.values              # This returns the values of the series.\n",
        "newlabel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "351ZLsMxduYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(reviews)\n",
        "print(newlabel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz1nYnn0BevX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Splitting dataset into Train, validation and Test.\n",
        "\n",
        "total=len(reviews)\n",
        "trainratio , validratio, testratio = (0.7, 0.15, 0.15)\n",
        "\n",
        "sep1=int(trainratio*total)\n",
        "sep2=int(validratio*total)\n",
        "\n",
        "reviews_tr=reviews[:sep1]\n",
        "reviews_va=reviews[sep1:sep1+sep2]\n",
        "reviews_te=reviews[sep1+sep2:total]\n",
        "\n",
        "labels_tr=newlabel[:sep1]\n",
        "labels_va=newlabel[sep1:sep1+sep2]\n",
        "labels_te=newlabel[sep1+sep2:total]\n",
        "\n",
        "labels_te.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zeUAGU_dwU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating the proper dataset for feeding to the network by using \"tf.data.Dataset\" Module\n",
        "\n",
        "batchsize = 200\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((reviews_tr,labels_tr)).shuffle(1000).batch(batchsize)\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((reviews_va,labels_va)).shuffle(1000).batch(batchsize)\n",
        "test_ds  = tf.data.Dataset.from_tensor_slices((reviews_te,labels_te)).shuffle(1000).batch(batchsize)\n",
        "\n",
        "train_ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSV8v5D7Bj_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To Look at how the dataset looks like\n",
        "\n",
        "samp_ds  = tf.data.Dataset.from_tensor_slices((reviews[:10],newlabel[:10]))\n",
        "\n",
        "for samp in samp_ds:\n",
        "  print(samp)\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MALyYVIYDEEE",
        "colab_type": "text"
      },
      "source": [
        "<br><b><i>Building the Network</i></b>\n",
        "\n",
        "We used Keras API to build a Neural Network model.<br>\n",
        "The input pipeline and the network is built as shown below.<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGQIv7Nodzme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# url = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\".   Using the same Url as the text embedding module\n",
        "\n",
        "hub_layer=hub.KerasLayer(url, input_shape=[], dtype=tf.string, trainable=True)  # Creating a NN layer which acts as the input pipeline and vectorizes the input review\n",
        "                                                                                # Since it is true, The embedder will have to train all its parameters\n",
        "model = tf.keras.Sequential([\n",
        "                             hub_layer,                                         # The embedding layer mmentioned above\n",
        "                             tf.keras.layers.Dense(20,activation='relu'),       # A Hidden Dense layer with 16 nodes, with ReLU activation function\n",
        "                             tf.keras.layers.Dense(20,activation='relu'),       # A Hidden Dense layer with 16 nodes, with ReLU activation function\n",
        "                             tf.keras.layers.Dense(1,activation='tanh')         # A Dense layer with 1 node for the output. The Tanh activation gives an output of range (-1,1)\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm33c3_xd3VX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lets configure the learning process/algorithm\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='squared_hinge',\n",
        "    metrics='accuracy'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW8kJb-0X6Am",
        "colab_type": "text"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>The structure of the model can be seen below.</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAQfYoR4N_pM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=12zhO5I2DDjkpDNASj9qCA-n5V3E6jce5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyxQTH4ed1rr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "690jhr8yxt1u",
        "colab_type": "text"
      },
      "source": [
        "<br><b><i><u>Training the network</i></b><br></u>\n",
        "Note that we use the validation data for checking the model performance in between <i>each</i> epoch.<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6bUV7k0d484",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numepoch = 15\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "                    train_ds,\n",
        "                    epochs=numepoch, \n",
        "                    validation_data=valid_ds,\n",
        "                    verbose=1\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcf4iRpiB8jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting Training and validation accuracies to check underfitting or overfitting\n",
        "\n",
        "tracc=history.history['accuracy']\n",
        "valacc=history.history['val_accuracy']\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.ylim(0.5,1)                                         # Y axis ranging from 0.5 to 1.0 accuracy\n",
        "plt.plot(range(1,numepoch+1),tracc,label='Train')\n",
        "plt.plot(range(1,numepoch+1),valacc,label='Valid')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epochs   -->')\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "print('\\n \"The epoch with highest validation accuracy is ',np.argmax(valacc)+1,'\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4mcd906Y0Hl",
        "colab_type": "text"
      },
      "source": [
        "<br><b><i><u>Evaluating the model</i></b><br></u>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C96VoGsB-IK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, acc = model.evaluate(test_ds)\n",
        "print('The Model achieved an accuracy of: {} %'.format(acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLtAZxYRd6xY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(string):\n",
        "  prob=model.predict([string])\n",
        "  print('Model Output: ',prob[0][0],'\\n')\n",
        "\n",
        "  if prob[0][0]>0:\n",
        "    print(\"The Review is POSITIVE!\")\n",
        "  else:\n",
        "    print(\"The Review is NEGATIVE!\")\n",
        "  print('The strength of review is {}%'.format(abs(prob[0][0]*100)))\n",
        "\n",
        "rev1='As iam told that this movie is really good, i really didnt feel it was satisfactory. But compared to other films, this was not good'     # A mediocre review\n",
        "\n",
        "rev2='Oh My god!! This is the kind of movie i expect to watch. This Movie was so awesome and fantastic that I couldnt resist watching it again and again' # An Excellent review\n",
        "\n",
        "rev3='Ughh! Do you even call this a movie? What type of sane minded human watches this? I dont have words to decribe how awful this movie is'   # Worst review\n",
        "\n",
        "predict(rev1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWeaT-ItdIHW",
        "colab_type": "text"
      },
      "source": [
        "#<i>Evaluation of the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLXUrwgDOzRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred=model.predict(reviews_te)\n",
        "\n",
        "print(\"Predictions: \\n\",y_pred,end='\\n\\n')          # Since the predictions are a continuous value, we need to convert them to categories of 0 and 1 (Just like the labels_te)\n",
        "\n",
        "pred_lab=[]\n",
        "for lab in y_pred:                                \n",
        "  if lab>=0:                                      # The predictions range from (-1,1), thus we seperate it by '0' value\n",
        "    pred_lab.append(1)\n",
        "  else:\n",
        "    pred_lab.append(0)\n",
        "\n",
        "print('Binary predictions= ',pred_lab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esfL5drZOX9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "\n",
        "print('Accuracy score: ',accuracy_score(pred_lab,labels_te),end='\\n\\n')\n",
        "\n",
        "print('The Confusion Matrix: \\n',confusion_matrix(pred_lab,labels_te),end='\\n\\n')\n",
        "\n",
        "print('The classification report: \\n',classification_report(pred_lab,labels_te,target_names=['Negative','Positive'],digits=2))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}